{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Medical Texts\n",
    "This notebook will use various vectorizers and deep learning methods to classify transcriptions of medical notes and text into various areas of medicine. The text has already been cleaned and preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "import random\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shru\\\\Documents\\\\Springboard\\\\Capstone 3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4966 entries, 0 to 4965\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   medical_specialty  4966 non-null   object\n",
      " 1   text               4966 non-null   object\n",
      " 2   class_label        4966 non-null   int64 \n",
      " 3   tokens             4966 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 155.3+ KB\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\Shru\\\\Documents\\\\Springboard\\\\Capstone 3/data'\n",
    "\n",
    "data = pd.read_csv(path+'/datafull.tsv', delimiter='\\t')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subjective patient admit shortness breath continue fairly well patient chronic atrial fibrillation anticoagulation inr 172 patient undergo echocardiogram show aortic stenosis severe patient outside cardiologist understand schedule undergo workup regard physical examination vital signs pulse 78 blood pressure 13060 lungs clear heart soft systolic murmur aortic area abdomen soft nontender extremities edema impression 1 status shortness breath respond well medical management 2 atrial fibrillation chronic anticoagulation 3 aortic stenosis recommendations 1 continue medication 2 patient would like follow cardiologist regard aortic stenosis may need surgical intervention regard explain patient discharge home medical management appointment see cardiologist next day interim change mind concern request call back'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][4634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og = pd.read_csv('medical_transcriptions/mtsamples.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4966 entries, 0 to 4965\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   medical_specialty  4966 non-null   object\n",
      " 1   transcription      4966 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 77.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop empty transcription values, drop uncessary columns for our modeling\n",
    "data_og = data_og.drop(data_og[data_og['transcription'].isna()].index).reset_index(drop=True)\n",
    "data_og = data_og[['medical_specialty','transcription']]\n",
    "data_og.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, clean_questions, generate_missing=False):\n",
    "    embeddings = clean_questions['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
    "                                                                                generate_missing=generate_missing))\n",
    "    return list(embeddings)\n",
    "\n",
    "def w2v(data):\n",
    "\n",
    "    embeddings = get_word2vec_embeddings(word2vec, data)\n",
    "    list_labels = data[\"class_label\"].tolist()\n",
    "    \n",
    "    return embeddings, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_v2(data, ngrams_l = 1, ngrams_u = 1):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(ngrams_l, ngrams_u))\n",
    "    tfidf_vectorizer.fit(data['text'])\n",
    "\n",
    "    list_corpus = data[\"text\"].tolist()\n",
    "    list_labels = data[\"labels\"].tolist()\n",
    "\n",
    "    X = tfidf_vectorizer.transform(list_corpus)\n",
    "    \n",
    "    return X, list_labels\n",
    "\n",
    "def w2v_v2(data):\n",
    "\n",
    "    embeddings = get_word2vec_embeddings(word2vec, data)\n",
    "    list_labels = data[\"labels\"].tolist()\n",
    "    \n",
    "    return embeddings, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data['medical_specialty'].value_counts()\n",
    "data_adj = data.copy(deep=True)\n",
    "data_adj.loc[data_adj['medical_specialty'].isin(counts[counts<100].index), 'medical_specialty'] = ' Other Specialties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Surgery                          1088\n",
       " Other Specialties                1072\n",
       " Consult - History and Phy.        516\n",
       " Cardiovascular / Pulmonary        371\n",
       " Orthopedic                        355\n",
       " Radiology                         273\n",
       " General Medicine                  259\n",
       " Gastroenterology                  224\n",
       " Neurology                         223\n",
       " SOAP / Chart / Progress Notes     166\n",
       " Urology                           156\n",
       " Obstetrics / Gynecology           155\n",
       " Discharge Summary                 108\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_adj['medical_specialty'].unique())\n",
    "data_adj['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data_og['medical_specialty'].value_counts()\n",
    "df = data_og.copy(deep=True)\n",
    "df.loc[df['medical_specialty'].isin(counts[counts<100].index), 'medical_specialty'] = ' Other Specialties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Surgery                          1088\n",
       " Other Specialties                1072\n",
       " Consult - History and Phy.        516\n",
       " Cardiovascular / Pulmonary        371\n",
       " Orthopedic                        355\n",
       " Radiology                         273\n",
       " General Medicine                  259\n",
       " Gastroenterology                  224\n",
       " Neurology                         223\n",
       " SOAP / Chart / Progress Notes     166\n",
       " Urology                           156\n",
       " Obstetrics / Gynecology           155\n",
       " Discharge Summary                 108\n",
       "Name: medical_specialty, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['medical_specialty'].unique())\n",
    "df['medical_specialty'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequential(input_size, output_size):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(64, activation = 'relu', input_shape=(input_size,)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(output_size, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_adj['text']\n",
    "le = LabelEncoder()\n",
    "le.fit(data_adj['medical_specialty'])\n",
    "y = le.transform(data_adj['medical_specialty'])\n",
    "output_size = len(le.classes_)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "x_train_vec = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "x_test_vec = tfidf_vectorizer.transform(X_test).toarray()\n",
    "y_train_vec=keras.utils.to_categorical(y_train, data_adj['medical_specialty'].nunique())\n",
    "y_test_vec=keras.utils.to_categorical(y_test, data_adj['medical_specialty'].nunique())\n",
    "\n",
    "n_cols=x_train_vec.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 5000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size, n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shru\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3972 samples, validate on 994 samples\n",
      "Epoch 1/30\n",
      "3972/3972 [==============================] - 1s 150us/step - loss: 2.4130 - accuracy: 0.2674 - val_loss: 2.2053 - val_accuracy: 0.3089\n",
      "Epoch 2/30\n",
      "3972/3972 [==============================] - 0s 112us/step - loss: 2.0788 - accuracy: 0.3316 - val_loss: 1.9174 - val_accuracy: 0.3431\n",
      "Epoch 3/30\n",
      "3972/3972 [==============================] - 0s 102us/step - loss: 1.8401 - accuracy: 0.3920 - val_loss: 1.7574 - val_accuracy: 0.3722\n",
      "Epoch 4/30\n",
      "3972/3972 [==============================] - 0s 102us/step - loss: 1.6906 - accuracy: 0.4134 - val_loss: 1.6547 - val_accuracy: 0.3702\n",
      "Epoch 5/30\n",
      "3972/3972 [==============================] - 0s 103us/step - loss: 1.5713 - accuracy: 0.4247 - val_loss: 1.5903 - val_accuracy: 0.3521\n",
      "Epoch 6/30\n",
      "3972/3972 [==============================] - 0s 103us/step - loss: 1.4866 - accuracy: 0.4355 - val_loss: 1.5496 - val_accuracy: 0.3260\n",
      "Epoch 7/30\n",
      "3972/3972 [==============================] - 0s 103us/step - loss: 1.4174 - accuracy: 0.4471 - val_loss: 1.5270 - val_accuracy: 0.3209\n",
      "Epoch 8/30\n",
      "3972/3972 [==============================] - 0s 103us/step - loss: 1.3667 - accuracy: 0.4441 - val_loss: 1.5148 - val_accuracy: 0.2857\n",
      "Epoch 9/30\n",
      "3972/3972 [==============================] - 0s 105us/step - loss: 1.3111 - accuracy: 0.4700 - val_loss: 1.5109 - val_accuracy: 0.2686\n",
      "Epoch 10/30\n",
      "3972/3972 [==============================] - 0s 104us/step - loss: 1.2608 - accuracy: 0.4826 - val_loss: 1.5139 - val_accuracy: 0.2465\n",
      "Epoch 11/30\n",
      "3972/3972 [==============================] - 0s 104us/step - loss: 1.2347 - accuracy: 0.4773 - val_loss: 1.5216 - val_accuracy: 0.2435\n",
      "Epoch 12/30\n",
      "3972/3972 [==============================] - 0s 104us/step - loss: 1.2044 - accuracy: 0.4982 - val_loss: 1.5338 - val_accuracy: 0.2163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1db80bf6e48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_sequential(n_cols, output_size)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size=100\n",
    "epochs=30\n",
    "\n",
    "# checkpoint=ModelCheckpoint('model-{epoch:03d}.model', monitor='val_loss', verbose=0, save_best_only=False, mode='auto')\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "model.fit(x_train_vec, y_train_vec, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, verbose=1, \n",
    "          validation_data = (x_test_vec, y_test_vec), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.constraints import maxnorm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22129 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data_adj['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4966, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(data_adj['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (4966, 13)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data_adj['medical_specialty']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3972, 250) (3972, 13)\n",
      "(994, 250) (994, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/5\n",
      "3574/3574 [==============================] - 17s 5ms/step - loss: 2.3430 - accuracy: 0.2429 - val_loss: 2.1942 - val_accuracy: 0.2965\n",
      "Epoch 2/5\n",
      "3574/3574 [==============================] - 17s 5ms/step - loss: 2.0815 - accuracy: 0.3111 - val_loss: 2.0219 - val_accuracy: 0.3015\n",
      "Epoch 3/5\n",
      "3574/3574 [==============================] - 18s 5ms/step - loss: 1.9966 - accuracy: 0.3304 - val_loss: 2.2062 - val_accuracy: 0.2588\n",
      "Epoch 4/5\n",
      "3574/3574 [==============================] - 19s 5ms/step - loss: 1.9045 - accuracy: 0.3598 - val_loss: 1.9734 - val_accuracy: 0.3392\n",
      "Epoch 5/5\n",
      "3574/3574 [==============================] - 20s 6ms/step - loss: 1.8156 - accuracy: 0.3959 - val_loss: 1.9974 - val_accuracy: 0.3266\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994/994 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 1.984\n",
      "  Accuracy: 0.318\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/30\n",
      "3574/3574 [==============================] - 38s 11ms/step - loss: 2.3013 - accuracy: 0.2471 - val_loss: 2.0845 - val_accuracy: 0.2940\n",
      "Epoch 2/30\n",
      "3574/3574 [==============================] - 40s 11ms/step - loss: 2.0104 - accuracy: 0.3262 - val_loss: 2.0398 - val_accuracy: 0.3317\n",
      "Epoch 3/30\n",
      "3574/3574 [==============================] - 39s 11ms/step - loss: 1.9041 - accuracy: 0.3632 - val_loss: 1.9767 - val_accuracy: 0.3317\n",
      "Epoch 4/30\n",
      "3574/3574 [==============================] - 39s 11ms/step - loss: 1.7768 - accuracy: 0.3833 - val_loss: 1.9619 - val_accuracy: 0.3090\n",
      "Epoch 5/30\n",
      "3574/3574 [==============================] - 40s 11ms/step - loss: 1.6661 - accuracy: 0.4113 - val_loss: 2.0715 - val_accuracy: 0.2940\n",
      "Epoch 6/30\n",
      "3574/3574 [==============================] - 41s 11ms/step - loss: 1.5870 - accuracy: 0.4177 - val_loss: 2.0277 - val_accuracy: 0.2688\n",
      "Epoch 7/30\n",
      "3574/3574 [==============================] - 39s 11ms/step - loss: 1.5046 - accuracy: 0.4342 - val_loss: 2.1995 - val_accuracy: 0.2864\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, return_sequences=True, kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994/994 [==============================] - 2s 2ms/step\n",
      "Test set\n",
      "  Loss: 2.111\n",
      "  Accuracy: 0.309\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22780 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['transcription'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4966, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['transcription'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (4966, 13)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['medical_specialty']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3972, 250) (3972, 13)\n",
      "(994, 250) (994, 13)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/5\n",
      "3574/3574 [==============================] - 17s 5ms/step - loss: 2.3582 - accuracy: 0.2269 - val_loss: 2.2581 - val_accuracy: 0.2111\n",
      "Epoch 2/5\n",
      "3574/3574 [==============================] - 18s 5ms/step - loss: 2.1998 - accuracy: 0.2762 - val_loss: 2.2695 - val_accuracy: 0.2060\n",
      "Epoch 3/5\n",
      "3574/3574 [==============================] - 19s 5ms/step - loss: 2.1171 - accuracy: 0.2991 - val_loss: 2.2123 - val_accuracy: 0.2538\n",
      "Epoch 4/5\n",
      "3574/3574 [==============================] - 19s 5ms/step - loss: 1.9977 - accuracy: 0.3470 - val_loss: 2.0209 - val_accuracy: 0.3141\n",
      "Epoch 5/5\n",
      "3574/3574 [==============================] - 20s 6ms/step - loss: 1.8786 - accuracy: 0.3786 - val_loss: 1.9712 - val_accuracy: 0.3291\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994/994 [==============================] - 1s 1ms/step\n",
      "Test set\n",
      "  Loss: 1.922\n",
      "  Accuracy: 0.361\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/30\n",
      "3574/3574 [==============================] - 23s 6ms/step - loss: 2.3020 - accuracy: 0.2487 - val_loss: 2.1544 - val_accuracy: 0.2940\n",
      "Epoch 2/30\n",
      "3574/3574 [==============================] - 24s 7ms/step - loss: 2.0630 - accuracy: 0.3148 - val_loss: 2.0411 - val_accuracy: 0.3191\n",
      "Epoch 3/30\n",
      "3574/3574 [==============================] - 25s 7ms/step - loss: 1.9647 - accuracy: 0.3492 - val_loss: 1.9807 - val_accuracy: 0.3618\n",
      "Epoch 4/30\n",
      "3574/3574 [==============================] - 26s 7ms/step - loss: 1.8736 - accuracy: 0.3763 - val_loss: 1.9758 - val_accuracy: 0.3191\n",
      "Epoch 5/30\n",
      "3574/3574 [==============================] - 27s 8ms/step - loss: 1.8086 - accuracy: 0.3830 - val_loss: 1.9878 - val_accuracy: 0.3467\n",
      "Epoch 6/30\n",
      "3574/3574 [==============================] - 27s 7ms/step - loss: 1.7627 - accuracy: 0.3959 - val_loss: 2.0155 - val_accuracy: 0.3090\n",
      "Epoch 7/30\n",
      "3574/3574 [==============================] - 27s 7ms/step - loss: 1.7127 - accuracy: 0.4077 - val_loss: 2.0268 - val_accuracy: 0.2990\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994/994 [==============================] - 4s 4ms/step\n",
      "Test set\n",
      "  Loss: 1.991\n",
      "  Accuracy: 0.312\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/30\n",
      "3574/3574 [==============================] - 14s 4ms/step - loss: 2.3605 - accuracy: 0.2306 - val_loss: 2.1928 - val_accuracy: 0.2764\n",
      "Epoch 2/30\n",
      "3574/3574 [==============================] - 13s 4ms/step - loss: 2.1031 - accuracy: 0.3036 - val_loss: 2.0608 - val_accuracy: 0.2915\n",
      "Epoch 3/30\n",
      "3574/3574 [==============================] - 15s 4ms/step - loss: 2.0361 - accuracy: 0.3248 - val_loss: 2.0623 - val_accuracy: 0.3116\n",
      "Epoch 4/30\n",
      "3574/3574 [==============================] - 15s 4ms/step - loss: 1.9351 - accuracy: 0.3472 - val_loss: 2.0457 - val_accuracy: 0.3116\n",
      "Epoch 5/30\n",
      "3574/3574 [==============================] - 15s 4ms/step - loss: 1.8432 - accuracy: 0.3797 - val_loss: 2.0373 - val_accuracy: 0.3015\n",
      "Epoch 6/30\n",
      "3574/3574 [==============================] - 16s 5ms/step - loss: 1.7497 - accuracy: 0.4068 - val_loss: 2.0579 - val_accuracy: 0.2864\n",
      "Epoch 7/30\n",
      "3574/3574 [==============================] - 16s 4ms/step - loss: 1.6785 - accuracy: 0.4183 - val_loss: 2.0936 - val_accuracy: 0.2990\n",
      "Epoch 8/30\n",
      "3574/3574 [==============================] - 15s 4ms/step - loss: 1.6141 - accuracy: 0.4440 - val_loss: 2.1094 - val_accuracy: 0.2588\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using TensorFlow version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"You are using TensorFlow version\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello=tf.constant('Hello,TensorFlow!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello,TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3574 samples, validate on 398 samples\n",
      "Epoch 1/30\n",
      "3574/3574 [==============================] - 43s 12ms/step - loss: 2.3006 - accuracy: 0.2504 - val_loss: 2.0870 - val_accuracy: 0.3166\n",
      "Epoch 2/30\n",
      "3574/3574 [==============================] - 49s 14ms/step - loss: 2.0774 - accuracy: 0.3232 - val_loss: 2.1066 - val_accuracy: 0.2764\n",
      "Epoch 3/30\n",
      "3574/3574 [==============================] - 51s 14ms/step - loss: 2.0326 - accuracy: 0.3397 - val_loss: 2.1366 - val_accuracy: 0.2764\n",
      "Epoch 4/30\n",
      "3574/3574 [==============================] - 43s 12ms/step - loss: 2.0464 - accuracy: 0.3338 - val_loss: 2.1724 - val_accuracy: 0.2613\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994/994 [==============================] - 6s 6ms/step\n",
      "Test set\n",
      "  Loss: 2.073\n",
      "  Accuracy: 0.315\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "After all that testing the LSTM deep learning models could not perform better on the tokenized data than logistic regression performed on pca reduced tf-idf vectors. the best model using LSTM had 36% accuracy on the test data while Lasso LogReg obtained 38% accuracy on the test data. From theses experiments it can be seen that preprocessing methods heavily influence the learning of the model. Optimizations can be further explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
